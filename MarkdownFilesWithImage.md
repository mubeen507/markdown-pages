![image](https://user-images.githubusercontent.com/130965749/236374627-8661df04-1060-4a8b-85d2-7de803c71021.png)
## **DevOps Core:**

1. Fundamentals of DevOps
2. Linux
3. Git, GitHub
4. Docker
5. Kubernetes
6. SonarQube
7. Nexus Repos

**Fundamentals of IT & DevOps:**

1. What is Application
2. What is SDLC, ALM
3. SDLC Methodologies
4. What is Agile
5. What is Scrum
6. Azure Boards for Plan,Track,Discuss

What is Application ?

**Software Program for Specific Task**

1. Desktop Application
2. Web Application
3. Mobile Application
4. IOT Application

**LMS Application:**

1. Web Application for Learners
2. Admin App for Admins
3. Mobile APP - Android
4. Mobile APP - IOS

**LMS Application Stack:**

**Programming Languages & Frameworks**

HTML, CSS, JS

React JS- FrontEnd Framework

Express JS - Node JS - Backend

PostGres DB

Software Development Life Cycle (Application Lifecycle Management)

![image](https://user-images.githubusercontent.com/130965749/236374743-6200f058-3669-4ea9-a26a-44cd81655b90.png)

**Analysis:**

**LMS Application:**

1. Web Application for Learners
2. Admin App for Admins
3. Mobile APP - Android
4. Mobile APP - IOS

**List of features: KonaLMS (initiative)**

1. **Web Application ( Epic 1)**

  1. HomePage
  2. SignIn
  3. Landing Page
  4. Course Page
  5. ChatBot
  6. Resources
  7. Interviews
  8. Resumes
  9. Assignments
  10. Profile Page

**Design:**

UX Designer : Figma, Adobe XD

**Enterprise Architect:**

**Programming Languages & Frameworks:**

HTML, CSS, JS

React JS,Express JS - Node JS – Java ( More Robust), Py

PostGres DB

Version/Source Control: GIT & GITHUB

Code Quality: Sonarcube

Operating System: Linux

Containerization: Docker

Orchestration : Kubernetes, AKS OR EKS

Cloud: AWS Or Azure

CI/CD: Azure Devops, Jenkins , AWS DevOps, Git Actions

Configuration Management (IAAC): Terraform , Ansible

Monitoring: Azure Monitoring , AWS Cloud Watch , Prometheus & Grafana

**Development:**

**Dev Develops code in Local System:**

**VSCode:**

[https://code.visualstudio.com](https://code.visualstudio.com/)

Git: [https://git-scm.com/download/win](https://git-scm.com/download/win)

GITHUB: [https://github.com](https://github.com/)

Azure DevOps ( Boards) : https://dev.azure.com

Azure Portal ( Azure Cloud ) : https://portal.azure.com

LMS Application Code:

[https://github.com/KonaMarsTech/lms-public](https://github.com/KonaMarsTech/lms-public)

**Testing:**

Software testing is the process of evaluating and verifying that a software product or application does what it is supposed to do. The benefits of testing include preventing bugs, reducing development costs and improving performance.

**Deployment:**

A deployment in general refers to the process of releasing and updating software applications, services, and infrastructure in a controlled and predictable manner. It involves creating, testing, and delivering software updates to various environments such as development, staging, and production. The goal of a deployment is to ensure that new changes are made available to end-users in a reliable and consistent manner

[https://digital-lync.konalms.com/](https://digital-lync.konalms.com/) - 50,000

[https://digital-lync.dev.konalms.com/](https://digital-lync.dev.konalms.com/) 5 Development

[https://digital-lync.qa.konalms.com/](https://digital-lync.dev.konalms.com/) Qa -5

**Maintenance:**

Maintenance in the Software Development Life Cycle (SDLC) refers to the ongoing process of updating, improving, and managing software after it has been developed and deployed.

The maintenance phase of the SDLC includes various tasks such as bug fixing, security patching, feature enhancements, and performance optimization.

**SDLC Methodologies:**

![image](https://user-images.githubusercontent.com/130965749/236374883-531ee8fc-a9c7-4013-8bcf-21b88ef10033.png)

**Agile Methodology: (Value)**

![image](https://user-images.githubusercontent.com/130965749/236374963-9a5a5387-c086-46fb-8da0-fc8181ab1cf5.png)

![image](https://user-images.githubusercontent.com/130965749/236374963-9a5a5387-c086-46fb-8da0-fc8181ab1cf5.png) ![image](https://user-images.githubusercontent.com/130965749/236374963-9a5a5387-c086-46fb-8da0-fc8181ab1cf5.png) ![image](https://user-images.githubusercontent.com/130965749/236374963-9a5a5387-c086-46fb-8da0-fc8181ab1cf5.png)
Iteration 1 Iteration 2 Iteration 3

**LMS Application:**

**List of features: KonaLMS (initiative)**

1. Web Application Learners

- HomePage
- SignIn
- Landing Page
- Course Page
- ChatBot
- Resources
- Interviews
- Resumes
- Assignments
- Profile Page

1. Admin App for Admins
2. Mobile APP - Android
3. Mobile APP - IOS

**Iteration 1:**

HomePage

SignIn

Landing Page

![image](https://user-images.githubusercontent.com/130965749/236374963-9a5a5387-c086-46fb-8da0-fc8181ab1cf5.png)

**Iteration : 2**

Course Page

Resources

Interviews

![image](https://user-images.githubusercontent.com/130965749/236374963-9a5a5387-c086-46fb-8da0-fc8181ab1cf5.png)

**Iteration : 3**

- Resumes
- Assignments
- Profile Page

![image](https://user-images.githubusercontent.com/130965749/236374963-9a5a5387-c086-46fb-8da0-fc8181ab1cf5.png)

**SCRUM Framework:**

![image](https://user-images.githubusercontent.com/130965749/236375193-8a3bd181-67d6-452a-8b21-e309e7d211b7.png)

![image](https://user-images.githubusercontent.com/130965749/236375356-e47c4f9f-c2c0-4abd-9389-52bbbdb9ded8.png)
Scrum:

**Product Backlog:**

**List of features: KonaLMS (initiative)**

1. Web Application for Learners

- HomePage
- SignIn
- Landing Page
- Course Page
- ChatBot
- Resources
- Interviews
- Resumes
- Assignments
- Profile Page

1. Admin App for Admins
2. Mobile APP - Android
3. Mobile App - IOS

**Sprint backLog:**

Sprint: 1

- HomePage
- SignIn

Sprint: 2

- Landing Page
- ChatBot

Sprint: 3

- Course Page
- ChatBot
- Resources

Sprint: 4

- Interviews
- Resumes
- Assignments
- Profile Page

**Scrum Team: 11**

1. Product Owner
2. Business Analyst
3. Scrum Master
4. Designer -1
5. Frontend Dev
6. Backend Developer
7. Full Stack Dev
8. Tester (QA)
9. Cloud DevOps Engineers -2
![image](https://user-images.githubusercontent.com/130965749/236375454-f20aee7b-a9f6-4323-9507-634987c0f82f.png)

**Epics – Large chunks of work that can be divided into stories**

![image](https://user-images.githubusercontent.com/130965749/236375489-9633da95-457e-45c7-88ce-aab7badc83eb.png)

![image](https://user-images.githubusercontent.com/130965749/236375523-a2148b9b-fe29-4562-a6a1-1c0949067cb1.png)

**First SetUP GITHUB Account:**

[https://code.visualstudio.com](https://code.visualstudio.com/)

Git: [https://git-scm.com/download/win](https://git-scm.com/download/win)

GITHUB: [https://github.com](https://github.com/)

Azure DevOps ( Boards) : https://dev.azure.com

Azure Portal ( Azure Cloud ) : https://portal.azure.com

LMS Application Code:

[https://github.com/KonaMarsTech/lms-public](https://github.com/KonaMarsTech/lms-public)

**My DevOps Journey:**

Epic 1: Core DevOps

1. Fundamentals
2. Git & GitHub
3. Linux
4. LMS Application on Linux
5. Docker
6. Kubernetes
7. SonarQube
8. Nexus Repository

DC-Topic-01: Fundamentals of Application Lifecycle Management

- Application
- ALM ( SDLC)
- Agile
- Scrum
- Azure Boards

DC-US-01: Setup Platforms Required for ALM

- Install VSCode
- Install GIT
- Setup GITHUB
- Setup Azure DevOps
- Setup AWS Account
- Setup Azure Account
- Setup GCP Account

DC-IQ-01: Interview for Fundamentals

DC-Topic-02: Linux for DevOps

- What is Linux
- Linux Architecture
- Linux File System
- Linux Commands
- Linux Package Management
- Linux Operations

DC-US-03: Setup Linux VM on Azure, AWS, GCP

DC-Topic-03:: GIT & GitHub

- Installing GIT
- Git Setup on local
- Git Setup on Linux
- Git Branches
- Git Operations
- SetUp GITHUB
- GITHUB Operations

DC-US-02: Setup LMS GIT in Local and Collaborate with GitHub

- Clone LMS Code in local
- Make Changes to LMS Code and Test

DC-Topic-04: LMS Application on Linux VM

- What is Sever
- What is WebServer
- What is Nginx
- Nginx on Linux
- Three Tier Application (LMS Application)

DC-US-04 : Setup LMS Application on Linux VM

DC-Topic-05: Containerization with Docker

- Monolith Vs Microservices
- Evolution of Docker
- What is Docker
- Why Docker
- Docker Architecture
- Docker Image
- Docker Containers
- Docker File
- Docker Network
- Docker Volumes
- What is YAML
- Docker Compose
- Advanced Docker

DC-US-05: Setup LMS Application with Docker & Docker Compose

DC-Topic-06: Orchestration with Kubernetes

- What is Kubernetes
- Why Kubernetes
- Kubernetes Architecture
- Kubernetes Objects
- YAML For Kubernetes
- Labels & Selectors
- Namespaces
- Networking

DC-US-06: Setup LMS Application with Kubernetes

DC-Topic-07: Sonorqube

- SonarQube Introduction
- Software Quality
- Static Code Analysis
- Technical Debt
- Static and Dynamic Code Reviews

DC-US-07: SonarQube - Setup

- Setup Sonarcube
- Do Quick Code Analysis

DC-Topic-08: Nexus Repository

- Sonatype Nexus Introduction
- Artifacts Management

US 1: Nexus Setup

- Install Nexus Repository Manager
- Create Nexus Repository

US 2: Nexus Integrations

- Store Java Project Artifacts in Nexus
- Store Docker Images in Nexus
- Nexus Integration with Jenkins

Capstone Project: LMS V2
![image](https://user-images.githubusercontent.com/130965749/236375615-1c56b47b-920a-4b42-94f1-acb484f20c85.png)

## SSH keys are a pair of cryptographic keys used to authenticate and secure remote connections to a server or computer. The pair consists of a public key and a private key, which are mathematically related but cannot be derived from each other.

## When a user attempts to connect to a remote server using SSH, the server sends a challenge to the user. The user then signs the challenge with their private key and sends the signed response back to the server. The server then verifies the signature using the user's public key, which it already has on file. If the signature is valid, the server grants the user access.

## SSH keys are considered more secure than passwords because they are nearly impossible to guess or crack. They are commonly used by system administrators, developers, and other IT professionals to securely access remote servers and manage them.

**To Generate Keys we use the following commands.**

**Local System:**

ssh-keygen

**What is Server:**

A server is a computer or system that provides resources or services to other computers or systems over a network. Servers are typically more powerful and have more storage capacity than regular desktop computers, as they need to handle multiple requests simultaneously. They are also designed to run continuously without interruption.

![image](https://user-images.githubusercontent.com/130965749/236375750-01861212-172d-4021-9ffc-3d8250cade39.png)
![image](https://user-images.githubusercontent.com/130965749/236375777-04a29d6d-3e88-4fba-970d-6450872cab5b.png)

# **Linux Basics**

## **Introduction to Linux OS**

- Today, **Linux** runs many of the technologies that power up devices and services. From mobile phones, Google applications, social media networks, to GPS services, Internet of Things (IoT) and Artificial Intelligence (AI) products etc.

- All of the **fastest 500 supercomputers** in the world run on Linux.

- **96.3 percent of the top 1 million web servers** run on Linux OS.

- Since DevOps teams share many responsibilities, A DevOps engineer who knows how to configure/work with linux operating systems and networking technologies, will potentially have few software delivery obstacles.

- Knowing how to work with Linux OS is essential for DevOps to a continual and speedy software delivery process.

## **Linux Architecture:**

![image](https://user-images.githubusercontent.com/130965749/236375833-7adda3a0-1853-4991-aecc-38ec1ca0dc10.png)

**Kernel & Shell**

- Kernel: Central module(Heart) of OS, interacts with Hardware and responsible for memory management, process management etc.

- Shell : Shell is a command line interpreter i.e, translates commands entered by the user and converts them into a language that is understood by Kernel. Shell is an interface between user and kernel

# **Topic 2:**

## **Linux File System**

The Linux directory structure is like a tree. The base of the Linux file system hierarchy begins at the **root (/)**. Directories branch off the root, but everything starts at root.

The directory separator in Linux is the forward slash (/). When talking about directories and directory paths, "forward slash" is abbreviated to "slash." Often the root of the file system is referred to as " **slash**" since the full path to it is /.

If you hear someone say "look in slash" or "that file is in slash," they are referring to the root directory.

![image](https://user-images.githubusercontent.com/130965749/236375898-b278560e-e6bc-48e0-abcf-a2c945c5cb92.png)

**File System Structure**

- **bin** - The /bin directory is where you will find binary or executable files. Programs needed to perform basic tasks, i.e. copy, move, remove etc
- **etc** - Configuration files live in the /etc directory. Configuration files control how the operating system or applications behave.
- **dev** – The /dev directory contains device files that represent hardware devices of the server
- **home** - The /home directories allow each user to separate their data from the other users on the system, contains private directories of users
- **mnt** - Mount point for external drives connected to this computer, i.e. CDs or USB keys
- **tmp** - temporary files, clear the contents of /tmp at boot time. The /tmp directory is a great place to store temporary files
- **var** - variable data produced by programs, like logs are stored
- **opt -** The /opt directory houses optional or third party software.

![image](https://user-images.githubusercontent.com/130965749/236375937-01081ff5-388a-4421-a435-b9cdc1a0bd4d.png)

## **Navigation In CLI**

In windows based machines it's quite easy to navigate as we have Explorer.

Let's check how you can move around the linux system using CLI.

\> ls /

**pwd -**

So where are we now ? I mean which directory we are working in now ?

We are going to check **pwd** which stands for **Print Working Directory**.

pwd tells you what your current or present working directory is

\> **pwd**

When you first log on to a Linux system, the working directory is set to your home directory. This is where you put your files. On most systems, your home directory will be called /home/your\_user\_name i.e in our case **/home/azureuser**.

**cd -**

The most basic command of all time, **cd** (change directory). In order to move around in the system we use the cd command. Typing cd alone without any argument will bring you to the home directory of the user.

\> **cd**

- **~** (tilde) means the user's home directory, usually /home/username
- **.** (dot) means the current directory you're in
- **..** (dot dot) means the parent directory of the current directory you're in.

\> cd ~

\> cd /var/log

**ls -**

It's one thing to know where we are. Next we want to know what's in there ?

The command for this task is **ls**. It's short for list. List the contents of a directory.

\> **ls**

\> **ls /etc**

**Flags**

In addition to performing a general task, a command may also contain flags to specify a specific task you want the command to do. A flag is anything prefixed with a dash ( **-** ) that follows the command name.

For example, you can type **ls -l**. In this case, **l** being the flag, to display the content of the directory in a list view.

Flags can also be combined. **ls -a** display all content, including hidden files (files that are prefixed with a dot). When used with **ls -l** , we can combine them like this **ls -la**

##


## **vi Text Editor**

The **vi** editor is the most popular and **classic command line text editor** in the Linux family. Below, are some reasons which make it a widely used editor:

- It's pre installed in almost all Linux Distributions i.e centos, ubuntu etc
- It's portable i.e it works the same across different platforms
- It is user-friendly, used by millions of linux users

**Insert Mode**

This mode is for inserting text in the file. You can switch to the Insert mode from the command mode **by pressing 'i' on the keyboard**

Once you are in Insert mode, any key would be taken as an input for the file on which you are currently working. To return to the command mode and save the changes you must press the **esc** key

To launch the vim editor

\> **vi \<new-file\> or \<existing-file\>**

Create a new file named **web.conf** generwally .conf indicates configuration files

\> vi web.conf

To edit the file we need to switch to insert mode **by pressing 'i' on the keyboard** You can tell when you are in insert mode by looking at the bottom left corner.

Now type in a few lines of text and **press esc** which will take you back to edit mode. Command beginning with a colon **( : )** requires you to **hit \<enter\>** to complete t:q!

## **sudo**

The **sudo** command stands for " **super user do**!"

If you **prefix**" **sudo**" with any linux command, it will run that command with **elevated privileges**. Elevated privileges are required to perform certain administrative tasks.

\> **sudo command**

## **File Management:**

### **COPY**
```
- File
  - cp \<source-file\> \<dest-path\>
  ```
  ```
- Directory
  - cp -r \<source-dir\> \<dest-path\>
```
### **REMOVE**
```
- File
  - rm \<source-file\>
  ```
  ```
- Directory
  - rm -r \<source-dir\>
```
**RENAME / MOVE**
```
- Rename
  - mv \<old-file\> \<new-file-name\>
```
```
- Move
  - mv \<old-file\> \<dest-directory\>
```
**Package Manager:**

In Ubuntu, the default package manager is called "apt" (short for "Advanced Package Tool"). It is a command-line tool used to manage packages on Ubuntu and other Debian-based Linux distributions.

To use apt, open a terminal window and enter commands using the following syntax:

To update the package list:

sudo apt update

sudo apt install package-name

sudo apt install

**Version Control:**

**Version Control** , also known as **revision control** or **source control** ,

Version control is a system that helps to keep track of changes made to a set of files or code over time. It allows multiple people to work on the same set of files without overwriting each other's changes.

version control is like a time machine that keeps a record of every change made to a file or set of files. This means that you can easily go back in time to a previous version of a file, compare different versions, and see who made what changes and when. It is commonly used by software developers, but it can also be used for any type of document or file that undergoes multiple revisions.

![image](https://user-images.githubusercontent.com/130965749/236376146-b8f26d93-29b8-4fcf-a531-ef5608ff2dde.png)

- Version control allows for the ability to revert a document to a previous version.

![image](https://user-images.githubusercontent.com/130965749/236376180-0e0faa19-a409-4823-839d-2b22c00c59a9.png)
![image](https://user-images.githubusercontent.com/130965749/236376210-530a37f6-c487-4752-a91f-2f98c903d52f.png)

**Centralized Version Control System**

![image](https://user-images.githubusercontent.com/130965749/236376252-67a054ee-5035-4f56-8e42-b7e3dd5f701e.png)

![image](https://user-images.githubusercontent.com/130965749/236376284-262c7aab-4f16-4736-a0f8-aaba53c5fdf5.png)

**Distributed Version Control System**

![image](https://user-images.githubusercontent.com/130965749/236376313-932af1ad-a359-4bc8-b378-4fb73bd9d679.png)

![image](https://user-images.githubusercontent.com/130965749/236376342-210bba91-a9cb-4b3b-83d3-e3e87271dc8c.png)

**WHAT IS GIT**

Git is a free and open-source version control system that helps developers to manage and track changes in their code over time

Git is like a tool that allows developers to keep track of the changes they make to their code over time. It keeps a record of every change made to a file, allowing developers to easily undo changes or go back to an earlier version of their code. Git also allows multiple developers to work on the same project at the same time without interfering with each other's work.

**What is GITHUB ?**

GitHub is a web-based platform that allows developers to store and share their code with others. It is built on top of Git

It provides a central place to store and manage code repositories, and offers tools for version control, bug tracking, code review

![image](https://user-images.githubusercontent.com/130965749/236376393-3181e5ae-2ae3-4542-ac02-78a90b607b2c.png)

Repository is like a folder or directory that contains all the files and code related to a project, along with a record of every change made to those file

## **GIT WORKFLOW:**

![image](https://user-images.githubusercontent.com/130965749/236376422-aebd09d5-ffe8-4e5a-916b-ae2f49fc86a6.png)

**GIT Commands:**

- The **git add** command - adds a change in the working directory to the staging area. It tells Git that you want to include updates to a particular file in the next commit.

- The **git commit** command - creates a commit, which is like a snapshot of your repository. These commits are snapshots of your entire repository at specific times. You should make new commits often, based around logical units of change. Over time, commits should tell a story of the history of your repository and how it came to be the way that it currently is. Commits include lots of metadata in addition to the contents and message, like the author, timestamp, and more.
  - git commit -m "descriptive commit message"

- The **git push** command - The git push command is used to upload local repository content to a remote repository. Pushing is how you transfer commits from your local repository to a remote repo.
```
gitconfig--globaluser.name"kona"

gitconfig--globaluser.email"kona@konalms.com"

gitStatus

git --version

git add

git commit -m "Message"

git status

git config --list

git pull

git push

git remote set-url origin git@github.com:digital-edify/lms-public.git
```

## **BRANCHES**

**Git Branching**

- In a collaborative environment, it is common for several developers to share and work on the same source code.

- Some developers will be fixing bugs while others would be implementing new features. Therefore, there has got to be a **manageable way to maintain different versions of the same code base**.

- This is where the branch function comes to the rescue. **Branch allows each developer to branch out from the original code base and isolate their work from others**. Another good thing about branch is that it helps Git to easily merge the versions later on.

- It is a common practice to create a new branch for each task
  - (eg. bug fixing, new features etc.)

- Branching means you diverge from the main line(master - working copy of application) of development and continue to do work without messing with that main line.

![image](https://user-images.githubusercontent.com/130965749/236376467-05ca88ff-9c0b-4588-a6b8-aa6aea8c25ca.png)

![image](https://user-images.githubusercontent.com/130965749/236376492-9197b5ac-150e-4cc4-ad39-9976558aeb98.png)

Git is maintained by Linux.

Github is maintained by Microsoft.

Git is a command-line tool.

Github is a graphical user Interface.

Git is a Software

Github is a Service

Git was released in 2005.

Github was launched in 2008.

Git is Installed locally on the system.

Github is hosted on the web.

Git has no user management feature.

Github has built-in user management feature.

Git can manage source code history.

Github is a hosting service for git repositories.

Git is focused on version control & code sharing

Github is focused on centralized source code hosting

## **The LMS Application Stack:**

## **Three-Tier Architecture**

- Three-tier architecture is a well-established software application architecture that organizes applications into three logical tiers:
  - the frontend tier, or user interface;
  - the application tier, where data is processed;
  - the data tier, where the data associated with the application is stored and managed.

- For decades three-tier architecture was the prevailing architecture for client-server applications.

![image](https://user-images.githubusercontent.com/130965749/236376532-09f7d7b0-8c03-4d7f-aa69-44203543b973.png)

**Presentation Tier / Client-Side Programming**

- The frontend tier is the user interface and communication layer of the application, where the end user interacts with the application.

- Its main purpose is to display information to and collect information from the user. This top-level tier can run on a web browser, as a desktop application, or a graphical user interface (GUI), for example.

- Web presentation/Frontend tiers are usually developed using Hypertext Markup Language (HTML) and Cascading Style Sheets (CSS) and JavaScript.

- HTML tells a browser how to display the content of web pages, while CSS styles that content.

- JavaScript (JS). JS makes web pages interactive.

**Backend:**

the backend refers to the part of a software system or a website that handles the server-side functionalities, database management, and logic processing that are hidden from the user interface or front-end.

The backend is responsible for storing and retrieving data from the database, processing user requests, and generating responses that are sent back to the front-end for display.

Without the backend, the front-end would not be able to function properly as it would not have the necessary data and logic to operate. In other words, the backend is the backbone of any software application or website, providing the necessary support and functionality for the front-end to work seamlessly.

**Database:**

A database is an organized collection of data that is stored and managed in a structured way to allow for efficient retrieval and manipulation of information.

A database typically consists of tables, which contain rows of data that are organized into columns. Each column represents a different type of information, while each row contains a specific set of data related to that information. For example, a table for customer information might have columns for name, address, phone number, and email address, with each row representing a different customer.

Databases are essential for managing and storing data in a way that allows for easy retrieval and analysis. They are used in a variety of applications, including web and mobile applications

**An Overview of Nginx**

NGINX is open-source software for web serving, reverse proxying, caching, load balancing, media streaming, and more. It started out as a web server designed for maximum performance and stability

**What is an IP Address?**

An IP address (Internet Protocol address) is a numerical label assigned to every device connected to a computer network that uses the Internet Protocol for communication. It serves two main functions: identifying the host or network interface and providing the location of the host in the network.

**Versions of IP Address:**

IP addresses come in two versions: IPv4 (32-bit) and IPv6 (128-bit). They are usually written as a series of four numbers separated by dots for IPv4 (e.g. 192.168.1.1) and as eight groups of four hexadecimal digits separated by colons for IPv6 (e.g. 2001:0db8:85a3:0000:0000:8a2e:0370:7334).

**What are port numbers in linux**

In Linux, port numbers are 16-bit unsigned integers (0 to 65535) that are used to identify a specific process to which the data is to be sent once it reaches the host machine. Port numbers are assigned to well-known services such as HTTP (port 80), HTTPS (port 443), FTP (port 21),SSH Port (22) and others, as well as to custom applications running on a server. Network services running on a system listen to specific port numbers to receive incoming data and handle client requests.

**why we use port numbers**

Port numbers are used in computer networking to identify different applications or services running on a single host machine. The purpose of using port numbers is to allow multiple services or applications to run simultaneously on a single machine and to be reachable by external devices.

When a client wants to communicate with a service on a server, it sends data to the server's IP address and a specific port number. The operating system of the server then directs the incoming data to the appropriate service or application based on the port number. This way, multiple services or applications can share the same IP address but still be differentiated from each other based on the port number.

Therefore, port numbers allow multiple applications or services to coexist on the same machine, to be reachable from the network, and to be differentiated from each other for proper data routing and communication.

**What is a Web Server?**

A web server is a software application that runs on a computer and provides a platform for serving web content over the internet. It receives HTTP requests from clients (e.g., web browsers) and returns the corresponding HTML pages and other assets. Examples of popular web servers include Apache and Nginx.

![image](https://user-images.githubusercontent.com/130965749/236376568-53d2a313-4449-4d01-a577-4a3cb19bff7b.png)

**Why use Web Servers?**

Web servers are used for several reasons, including:

1. _ **Serving web content** _: Web servers are the primary means for serving HTML pages, images, videos, and other assets over the internet.
2. _ **Routing requests** _: Web servers can act as a reverse proxy, routing incoming requests to the appropriate backend servers based on the request URL.
3. _ **Load balancing** _: Web servers can distribute incoming requests across multiple servers for better performance and reliability.
4. _ **Security** _: Web servers can be configured to provide security features such as SSL encryption, IP blocking, and more.
5. _ **Scalability** _: Web servers can be easily scaled to handle increasing traffic, making them suitable for high-traffic websites.
6. _ **Accessibility** _: Web servers make it possible for people to access content and services from anywhere with an internet connection.

**Overview of Nginx**

- NGINX is pronounced as "engine-ex".
- It is an open-source, fast, lightweight, and high-performance web server that can be used to serve static files.
![image](https://user-images.githubusercontent.com/130965749/236376590-3251f72f-ad99-4877-884b-4cab6d6d0efc.png)
![image](https://user-images.githubusercontent.com/130965749/236376612-dae22768-a4a3-4945-bd3a-37b841b88331.png)

**Why use Nginx?**

Nginx is used for several reasons, including

- _ **Performance:** _ Nginx is known for its high performance and can handle a large number of concurrent connections efficiently.
- _ **Scalability:** _ Nginx can easily scale to handle increasing traffic, making it a good choice for high-traffic websites.
- _ **Reverse proxy:** _ Nginx can act as a reverse proxy, allowing it to route traffic from the internet to one or more backend servers.
- _ **Load balancing** _: Nginx can distribute incoming requests to multiple backend servers for better resource utilization and increased reliability
- _ **Security** _: Nginx can be configured to provide additional security features, such as SSL encryption and IP blocking.
- _ **Flexibility** _: Nginx is highly configurable and can be used for a variety of tasks, such as serving static files, proxying requests, and more.

**Install Nginx on Linux**

Here are the steps to install Nginx on an Ubuntu machine:

**Nginx file structure on Ubuntu**

On an Ubuntu system, the Nginx file structure is typically organized as follows:

**Configuration File:**
```
- /etc/nginx/ - Main Nginx configuration directory.
- /etc/nginx/nginx.conf - Main Nginx configuration file.
- /etc/nginx/sites-available/ - Directory for virtual host configurations.
- /etc/nginx/sites-enabled/ - Symbolic links to the virtual host configurations in sites-available.
- /etc/nginx/snippets/ - Directory for reusable configuration snippets.
```
**Log Files:**
```
/var/log/nginx/ - Directory for Nginx log files.

/var/log/nginx/access.log - Records all HTTP access requests.

/var/log/nginx/error.log - Records all Nginx error messages.
```
**Web root directory:**

/var/www/html/ - The default location for web content served by Nginx.

Here are some common Nginx commands on an Ubuntu system:

1. Start Nginx: sudo systemctl start nginx
2. Stop Nginx: sudo systemctl stop nginx
3. Restart Nginx: sudo systemctl restart nginx
4. Start Nginx: sudo systemctl start nginx
5. Nginx Reload: sudo nginx -s reload
6. Check Nginx Status: sudo systemctl status nginx
7. View Nginx Logs: sudo tail -f /var/log/nginx/error.log
8. Test Nginx: sudo nginx -t

**Default Configuration Files:**
```
/etc/nginx
```

**Logfiles:**
```
/var/log/nginx
```
**Content:**
```
/var/www/html

azureuser@VMLinuxServer1:/var/www/html$ cp -r /home/azureuser/lms-public/webapp/dist/\* .
```
**Nginx Config file for LMS:**

server {

server\_name [konastack.com](http://konastack.com/);

location / {

root /home/azureuser/lms-public/webapp/dist;

}

location /api {

proxy\_pass [http://localhost:8080](http://localhost:8080/);

}

}

This is a Nginx virtual host configuration block that defines a server named "konastack.com". The block contains two location blocks, which specify how Nginx should handle incoming HTTP requests based on the URL path.

1. The first location block with a root directory of /home/azureuser/lms-public/webapp/dist specifies that any requests to the root URL (e.g. konastack.com/) should be served from the specified directory. Nginx will look for files in this directory to serve in response to these requests.
2. The second location block with a proxy\_pass directive specifies that any requests to the /api URL (e.g. konastack.com/api) should be proxied to http://localhost:8080. This means that Nginx will forward these requests to the specified URL and return the response to the client. This is useful when you want to route requests to another server or application running on the same host.

This configuration block tells Nginx to listen for requests to the server named konastack.com and handle them as described above. This configuration block must be included in the main Nginx configuration file (usually /etc/nginx/nginx.conf) or in a separate file in the /etc/nginx/sites-available/ directory that is then linked to the /etc/nginx/sites-enabled/ directory.

/etc/nginx/sites-available/

/etc/nginx/sites-available/ - Directory for virtual host configurations.

/etc/nginx/sites-enabled/ - Symbolic links to the virtual host configurations in sites-available.

![image](https://user-images.githubusercontent.com/130965749/236376659-6817a433-ffb4-478e-a8ee-fffaea82b222.png)

![image](https://user-images.githubusercontent.com/130965749/236376684-3aabd6ba-eb41-44a6-9a70-640688987dcb.png)

![image](https://user-images.githubusercontent.com/130965749/236376731-69b5a3c2-a98d-46d8-8f34-ee67d89cd4b9.png)

1. Setup Azure VM

2. Connect to VM From VS Code

3. install Nginx

sudo apt install nginx

nginx -v

Verify Nginx : sudo systemctl status nginx

Check with IP: IP From Azure

4. Install Node JS Version 16.X

**curl -fsSL https://deb.nodesource.com/setup\_16.x | sudo -E bash - &&sudo apt-get install -y nodejs**

Verify Node is Installed : node -v

5. Install PostGres DB

https://www.postgresql.org/download/linux/ubuntu/

sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb\_release -cs)-pgdg main" \> /etc/apt/sources.list.d/pgdg.list'

wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -

sudo apt-get update

sudo apt-get -y install postgresql

6. setup DB Password : Password can be any thing -password

sudo su postgres

psql

command : \password

postgres=# \password

Enter new password for user "postgres":

Enter it again:

Exit : postgres=# exit

postgres@LMSVMServer:/home/azureuser$ exit

7. Setup LMS Code :

git clone [https://github.com/KonaMarsTech/lms-public.git](https://github.com/KonaMarsTech/lms-public.git)

Get Branches : git branch -r

Change the Branch to vm-docker-cicd

git checkout vm-docker-cicd

8. Get Nginx ready to Accept the Request for LMS Application

Stop Nginx : sudo systemctl stop nginx

Remove Defualt Config File :

sudo rm /etc/nginx/sites-enabled/default

sudo vi /etc/nginx/sites-available/lms-app

server {

server\_name konalms.in;

location / {

root /home/azureuser/lms-public/webapp/dist;

}

location /api {

proxy\_pass http://localhost:8080;

}

}

9. Godaddy and Point Domain Name to our Nginx

10.

Setup FE and FE

Got to WebApp Folder:

**npm install**

**npm install** is a command used in Node.js to install code libraries (also known as packages or dependencies) that your project needs to run properly. When you run npm install, it looks at a file called package.json in your project folder and installs all the libraries listed there. This command is important because it saves you time and effort in manually installing each library one by one, and ensures that you have all the necessary components for your project to function.

**npm run build**

npm run build is a command used in Node.js to prepare your web application for deployment on a production server. When you run this command, it creates a smaller and optimized version of your application that is ready to be uploaded to a web server or hosting service. This optimized version ensures that your application will load faster and perform better for your users.

11. Create a symlink for Sites Available to Sites Enabled

sudo ln -s /etc/nginx/sites-available/lms-app /etc/nginx/sites-enabled/lms-app

12 Restart Nginx to Access the Front

and Make URL is Up and Running

[http://yourdomain](http://yourdomain/)

13 Install Certificate to Enable HTTPS

URl : https://certbot.eff.org/instructions?ws=nginx&os=ubuntufocal

Select Ubuntu 20 and Nginx

sudo snap install core; sudo snap refresh core

sudo snap install --classic certbot

sudo ln -s /snap/bin/certbot /usr/bin/certbot

sudo certbot --nginx

Give Email ID :

Say : Y,Y

Select : 1

14: Set Backend API

Go to API Folder

Give DB info in Env File :

sudo vi .env

Go to Insert Mode by typing i

Paste the below code

MODE=production

PORT=8080

DATABASE\_URL=postgresql://postgres:password@localhost:5432/postgres

Exit with :wq

15: Get DataBase Ready

npx prisma db push

16 Build and Run Backend:

npm install

Install PM2 : Node Process Manager

sudo npm install -g pm2

npm run build

NODE\_PORT=8080 pm2 start -i 0 build/index.js

**User Story 2:**

FE : FE Container - Packaged application with Runtime

BE : BE Container

DB : PostGres Container

1. Install Linux VM on Azure
2. Install docker
3. Create Nginx Image
4. Create BE Image with Node
5. Create DB Images
6. We install Nginx and Http Certificate
7. Configure Domain Name
8. Make sure HTTPS is working

**Monolithic Application:**

A monolithic application is a type of software where all the different parts of the application are combined into a single, big program. This means that everything runs in the same process and changes to one part of the program can impact the whole program. In simpler terms, it's like having all the rooms of a house built as one big room, instead of having separate rooms for different functions.

**LMS Example:**

Monolithic application is a traditional Application, including the Login, ChatBot, Ask AI, Payment Gateway, Video Streaming, Caching Services is built as one big program. All the different parts of the Application are combined into a single codebase and run on a single server. When a user wants use the Application , they interact with different parts of the application, but it's all running as one program.

**Advantages of Monolithic Applications:**

1. Simple to develop: Monolithic applications are easier to develop as everything is in one codebase and developers can quickly navigate and make changes to the entire system.
2. Easy to understand: A monolithic application is easier to understand as everything is in one place, making it easier for new developers to get up to speed.
3. Lower costs: Monolithic applications typically have lower development and maintenance costs as there is no need for complex communication between separate services.

**Disadvantages of Monolithic Applications:**

1. Lack of scalability: As the size and complexity of the application grows, it becomes increasingly difficult to scale individual parts of the system.
2. Hard to maintain: As the application becomes more complex, it becomes increasingly difficult to maintain, test, and deploy individual parts of the system.
3. Difficult to make changes: Since everything is tightly-coupled in a monolithic application, making changes to one part of the system can impact the entire application, making it harder to make changes.
4. Limited fault tolerance: If one part of a monolithic application fails, it can bring down the entire application, making it less tolerant to faults.
5. Increased resource consumption: Monolithic applications can consume more memory and processing power as they are a single, large process, rather than many small processes.

**Microservices Application:**

Microservices is a way of building software applications where instead of building one big, complex program, you build many small, simple programs that work together to achieve the same goal. Each small program is called a "microservice" and is responsible for only one specific task. With LMS Application , you have one microservice for Login, , another for handling payments, and another for managing Videos. Each microservice runs on its own and can be updated and maintained independently, making the overall system more flexible and scalable.

Advantages of Microservice Architecture over Monolithic Architecture:

1. Scalability: Microservices can be deployed and scaled individually, allowing for easier scalability of individual parts of the system.
2. Resilience: If one microservice fails, the rest of the system can continue to function, making the system more resilient to faults.
3. Flexibility: Since each microservice is independent, it's easier to make changes to individual parts of the system without affecting the entire application.
4. Improved maintainability: Microservices can be developed, tested, and deployed independently, making it easier to maintain individual parts of the system.
5. Improved deployment: Microservices can be deployed more frequently, as only the affected microservices need to be redeployed, reducing downtime and improving the deployment process.
6. Technology heterogeneity: Different microservices can be developed using different programming languages and technology stacks, allowing for greater flexibility and adaptability to changing technologies.
7. Better resource utilization: Microservices consume fewer resources as they are smaller, lighter processes, rather than a single, large process.

**LMS Application with Microservices:**

1. OAuth Service - MS1
2. Vimeo Video Streaming
3. ChatBot
4. Ask AI
5. Payment GateWay
6. Certification
7. Online Meeting -Zoom
8. Redis for Cache

**Evolution on Docker:**
```
https://docs.docker.com/engine/install/ubuntu/
```

![image](https://user-images.githubusercontent.com/130965749/236376808-ef862817-c89f-41ec-8a18-6d76644d927a.png)

![image](https://user-images.githubusercontent.com/130965749/236376831-1d86b8ba-b3a2-4c07-99e6-3a1219d63692.png)

##
# **What are Hypervisors?**

A hypervisor is a crucial piece of software that makes virtualization possible. It creates a virtualization layer that separates the actual [hardware](https://phoenixnap.com/glossary/what-is-hardware) components - [processors](https://phoenixnap.com/kb/single-vs-dual-processors-server), RAM, and other physical resources - from the virtual machines and the operating systems they run.

The machine hosting a hypervisor is called the _host machine_, while the virtual instances running on top of the hypervisor are known as the _guest virtual machines_.

Hypervisors emulate available resources so that guest machines can use them. No matter what operating system boots up on a virtual machine, it will think that actual physical hardware is at its disposal.

###

###

### **Why Use a Hypervisor?**

From a VM's standpoint, there is no difference between the physical and virtualized environment. Guest machines do not know that the hypervisor created them in a virtual environment or that they share available computing power.

The fact that the hypervisor allows VMs to function as typical computing instances makes the hypervisor useful for companies planning to:

- Maximize utilization of their computing resources. Multiple virtual environments on a single server fully utilize the available CPU and memory.
- Provide better IT mobility. The VMs do not depend on their host hardware and can easily be transferred to another system.

### **Types of Hypervisors**

There are two types of hypervisors, according to their place in the server virtualization structure:

- Type 1 Hypervisors, also known as bare-metal or native.
- Type 2 Hypervisors, also known as hosted hypervisors.

![image](https://user-images.githubusercontent.com/130965749/236376874-c2fa598e-b0af-4436-9d56-a881e8891edd.png)

S ![image](https://user-images.githubusercontent.com/130965749/236376909-98229326-75af-4607-aadb-fd135cdc6e98.png)

![image](https://user-images.githubusercontent.com/130965749/236376932-bc9ef5d9-1b2f-43cf-a64e-4fcb0ea1c72a.png)

**What is Docker**

Docker is a popular platform that allows developers to package their software applications along with all the necessary dependencies and configurations, and run them in a portable and consistent manner across different environments such as local machines, servers, and cloud platforms.

Docker accomplishes this by providing a containerization technology that isolates the application from the underlying operating system, enabling developers to build, test, and deploy their software applications in a more efficient and streamlined way. Containers are lightweight and can be easily moved between different environments, making it easier to manage complex software applications and scale them as needed.

In simple terms, Docker helps developers create and manage containerized applications, which run consistently and reliably across different platforms and environments.

**Why Docker**

Docker provides several benefits that make it a popular choice among developers, system administrators, and DevOps teams:

1. Consistency: Docker allows developers to package their application along with all the dependencies and configurations into a single container image, ensuring that the application runs consistently and reliably across different environments.
2. Portability: Docker containers can be easily moved between different environments such as local machines, servers, and cloud platforms, enabling developers to deploy their applications anywhere with minimal changes.
3. Efficiency: Docker containers are lightweight and use minimal resources, making it possible to run more applications on the same infrastructure, thereby improving efficiency and reducing costs.
4. Isolation: Docker containers provide a high degree of isolation, enabling developers to run multiple applications on the same infrastructure without worrying about conflicts or compatibility issues.
5. Scalability: Docker makes it easier to scale applications up or down as needed, by enabling developers to quickly create new instances of containers and distribute them across different environments.

Overall, Docker provides a powerful set of tools for building, deploying, and managing containerized applications, which can help developers streamline their development process and improve the reliability and scalability of their applications.

**Docker Architecture:**

![image](https://user-images.githubusercontent.com/130965749/236376968-0bff3b4c-1fd6-4340-a71c-93ea8b4dab94.png)

Docker is a tool that helps you run your applications in containers. Containers are like mini-computers that run your applications and keep them isolated from the rest of your system. This way, you can run multiple applications on the same machine without them interfering with each other.

Docker has two main parts: the Docker client and the Docker daemon. The Docker client is what you use to interact with Docker and tell it what to do. The Docker daemon is the background process that actually builds and runs your containers.

When you use Docker, you start with a Docker image. An image is a pre-made package that includes everything your application needs to run.

Once you have an image, you can use it to create a Docker container. A container is a running instance of an image.

Finally, Docker images are stored in a Docker registry, which is like a library of images. You can use any image you find in the registry, or you can create your own and store it there for others to use.

Docker uses a client-server architecture. The Docker client communicates with the Docker daemon, which does the heavy lifting of building, running, and managing Docker containers. The Docker client and daemon can run on the same host, or the Docker client can connect to a remote Docker daemon.

Here's a high-level overview of the Docker architecture:

1. Docker client: This is the command-line tool that allows users to interact with Docker. The Docker client communicates with the Docker daemon to execute commands.
2. Docker daemon: This is the background process that runs on the host machine and does the heavy lifting of building, running, and managing containers. The Docker daemon communicates with the Docker client to receive commands and send status updates.
3. Docker images: Docker images are the building blocks of containers. They are pre-configured environments that contain everything needed to run a specific application. Docker images are created using a series of instructions called a Dockerfile.
4. Docker containers: Docker containers are instances of Docker images that are running as isolated processes. Containers are lightweight and fast, and they can be easily moved between hosts.
5. Docker registry: Docker registries are where Docker images are stored and distributed. The most popular Docker registry is Docker Hub, but you can also set up your own private registry.
6. Docker network: Docker provides a flexible and powerful networking system that allows containers to communicate with each other and with the host system.

This architecture allows for flexibility, scalability, and portability, making Docker a popular choice for developing and deploying applications in a variety of environments.

**Docker Daemon:**

The Docker daemon is the background process that runs on the host machine and is responsible for building, running, and managing Docker containers. The Docker daemon is responsible for carrying out the instructions from the Docker client and for communicating with the host operating system to manage containers and their resources.

The Docker daemon communicates with the Docker client over a REST API, and the client sends commands to the daemon to create, start, stop, and manage containers. The Docker daemon also communicates with the host operating system to allocate resources such as memory and disk space for the containers, and to manage network connections for the containers.

In short, the Docker daemon is the heart of the Docker system, responsible for executing the instructions from the Docker client and managing the containers. Without the Docker daemon, the Docker client wouldn't be able to perform any actions.

**Docker Client:**

Docker client is a command-line interface (CLI) that allows users to interact with the Docker daemon through the command line. The Docker client communicates with the Docker daemon to build, ship, and run containers. The Docker client and the Docker daemon can run on the same host, or the Docker client can connect to a remote Docker daemon. With the Docker client, you can manage containers, images, networks, and volumes, as well as automate deployment processes and perform other container-related tasks. The Docker client is an essential component of the Docker ecosystem and provides a convenient and powerful way to interact with Docker containers and services.

**Docker Images:**

A Docker image is a lightweight, stand-alone, and executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files. In simple terms, a Docker image is like a blueprint for a container. It contains all the information necessary to create a container, and once you run the image, you get a running container with the software and its dependencies.

Docker images are stored in a Docker registry, which is a centralized repository for storing and distributing Docker images. You can either use publicly available images from the Docker Hub registry or create your own images from scratch or based on existing images. The beauty of Docker images is that they are portable, meaning you can run them on any system that has Docker installed, regardless of the underlying operating system or hardware.

![image](https://user-images.githubusercontent.com/130965749/236377002-61e1e4c6-d029-4c6a-b6de-079aa2d3f1f2.png)

![image](https://user-images.githubusercontent.com/130965749/236377099-b1f7480e-7fbe-49a2-b078-19ef719cbd43.png)

**Docker Container:**

A Docker container is a standalone executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files. In simple terms, a Docker container is like a running instance of a Docker image.

When you run a Docker container, the Docker engine takes the image and creates a new, isolated environment for the software to run in. Each container runs in its own isolated environment, with its own file system, network, and processes, so you can run multiple containers on the same host without them interfering with each other.

The advantage of using Docker containers is that they provide a consistent and reproducible environment for software, so you can be sure that the code will run the same way, regardless of the environment it is run in. Containers are also lightweight and fast, making it easy to create, deploy, and manage multiple containers on a single host.

In summary, a Docker container is a running instance of a Docker image, providing a lightweight and isolated environment for the software to run in.

![image](https://user-images.githubusercontent.com/130965749/236377215-000cee7a-46ba-4ee2-9c56-439fc0443d2d.png)

**Docker Registry:**

A Docker registry is a centralized repository for storing and distributing Docker images. It acts as a library for Docker images, allowing users to store, share, and manage their images.

In simple terms, think of a Docker registry as a place where you can store and retrieve Docker images, just like you would store and retrieve books from a library. When you want to run a piece of software using Docker, you first need to find the relevant Docker image in a registry and then download it to your host. Once you have the image, you can run a Docker container using the image, and the software will run inside the container.

Docker registries can either be public, like the Docker Hub registry, or private, where only authorized users can access the images. Docker Hub is the largest public registry and is used by many organizations and individuals to store and distribute their images. However, organizations can also use their own private registries to store and manage images within their own infrastructure.

In summary, a Docker registry is a centralized repository for storing and distributing Docker images, allowing users to store, share, and manage their images.

**Docker Installation:**
![image](https://user-images.githubusercontent.com/130965749/236377313-b94343a6-bb72-4e83-bfe8-4b816da68e1e.png)

**Install Docker:**

[https://docs.docker.com/engine/install/ubuntu/](https://docs.docker.com/engine/install/ubuntu/)

### **Install using the repository**

Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.

#### **Set up the repository**

1. Update the apt package index and install packages to allow apt to use a repository over HTTPS:

sudo apt-get update
```
sudo apt-get install \
```
```
ca-certificates \
```
```
curl \
```
```
gnupg \
```
lsb-release

**Add Docker's official GPG key:**

sudo mkdir -m 0755 -p /etc/apt/keyrings

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg

Use the following command to set up the repository:

echo \

"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \

$(lsb\_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list \> /dev/null

####


#### 1.Install Docker Engine

1. Update the apt package index:

sudo apt-get update

2. Install Docker Engine, containerd, and Docker Compose.

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

##
# Linux post-installation steps for Docker Engine

These optional post-installation procedures shows you how to configure your Linux host machine to work better with Docker.

To create the docker group and add your user:

1. Create the docker group.

sudo groupadd docker

2. Add your user to the docker group.

sudo usermod -aG docker $USER

3. You can also run the following command to activate the changes to groups:

newgrp docker

**Docker Commands:**

1. docker run: Runs a command in a new container.
2. docker start: Starts one or more stopped containers.
3. docker stop: Stops one or more running containers.
4. docker ps: Lists containers and their status.
5. docker inspect: Returns low-level information on one or more containers or images.
6. docker logs: Shows logs from one or more containers.
7. docker pull: Downloads an image from a registry.
8. docker push: Uploads an image to a registry.
9. docker build: Builds an image from a Dockerfile.
10. docker images: Lists images.
11. docker rmi: Removes one or more images

**Nginx on Docker:**

![image](https://user-images.githubusercontent.com/130965749/236377480-10d02b89-1a91-44b8-8052-8f3136d8fbf4.png)

docker run --name lms nginx -p 80:80 -d nginx

Docker stop:
```
docker stop lms nginx
```
Docker Start:
```
docker start lms nginx
```
Docker Inspect:
```
docker inspect lms nginx
```
Docker logs:
```
docker logs lms nginx
```
**Docker Exec:**

**docker exec**** :** Runs a command in a running Nginx container,

docker exec -it lms nginx /bin/bash

The docker exec -it lms nginx /bin/bash command is used to run a command in a running Docker container. Here's what each part of the command does:

1. docker exec: This is the command used to run a command in a running Docker container.
2. -it: These flags allow you to run an interactive shell in the container, which means you can interact with the container in real-time. The -t flag allocates a pseudo-tty and the -i flag keeps stdin open.
3. mynginx: This is the name of the running Docker container you want to run the command in.
4. /bin/bash: This is the command you want to run in the container. In this case, you are running the Bash shell in the container, which allows you to access and run commands inside the container.

In simple terms, this command opens a Bash shell in the running Nginx container named "lms nginx", allowing you to interact with the container and run commands inside it.
![image](https://user-images.githubusercontent.com/130965749/236377537-7fb834e3-4343-4d2a-8209-d6d81f4a3148.png)

**Containers are ephemeral (ShortLived)**

In the context of Docker, "ephemeral" means that containers are designed to be disposable and replaceable. This is because containers are typically created from lightweight and immutable images, which can be easily rebuilt and redeployed if needed.

When a Docker container is stopped or deleted, all data and changes made to the container's file system are lost, unless they are saved to a persistent storage location such as a Docker volume. This means that any data or changes made within the container are not meant to be preserved for the long term, and should be treated as temporary.

By treating containers as ephemeral, Docker encourages a stateless and scalable approach to application deployment, where multiple instances of a container can be launched or destroyed on demand, allowing for greater flexibility and resource efficiency.

**What is Network:**

A network is a group of computers or devices that are connected together so they can share information and resources with each other. This could be a physical connection like a cable or a wireless connection like Wi-Fi.

Networks are useful because they allow people to communicate and share resources, like printers or files, between computers. Networks can also help people access the internet or connect to remote servers or cloud-based services.

Think of a network like a big web of connections between different devices. When they are connected, they can work together to share information and make things happen.

**Docker Network:**

In simple terms, a network in Docker is like a virtual bridge that allows communication between different Docker containers. Each network has its own unique address range, and containers on different networks cannot communicate directly with each other.

Think of it like a group of houses on a street. Each house has its own address, but they are all on the same street. Similarly, each Docker container has its own unique IP address, but they can all communicate with each other if they are on the same network.

Docker provides different types of networks with varying features and use cases, such as the default bridge network, which is like a basic connection between containers on the same machine, and user-defined networks, which allow for more customized settings and configurations.

In short, networks in Docker are important because they allow containers to communicate with each other and with other networks, which is essential for deploying containerized applications.

docker network create lms network

**Docker File:**

A Dockerfile is a set of instructions that tell Docker how to build a Docker image. A Docker image is a package that contains everything needed to run a piece of software, and Docker uses the instructions in the Dockerfile to assemble this package.

The Dockerfile contains information about what software and libraries need to be installed, what files should be included, and what commands should be run when the image is built. Once the Dockerfile is created, it can be used to build an image, which can then be used to create and run Docker containers.

The Dockerfile is an important tool for building custom images, which are used to create consistent, isolated runtime environments for software. By using Dockerfiles to build images, developers can ensure that their software runs the same way across different systems and environments, making it easier to deploy and manage their applications.

**Setup LMS ON Docker:**

1. Install VM on Cloud
2. Install Docker
3. Post Install Steps for Docker
4. SetUP DB
5. Create FE Images
6. Create FE Docker Files
7. Build Dokcer FE Image
8. Build Docker BE Image
9. External Nginx for HTTPS , LB
10. Docker Compose , YAML

**Create a DB:**

WithOut Network

docker run -d -p 5432:5432 -e POSTGRES\_PASSWORD=password --name lmsdb postgres

**With network:**

docker run -d -p 5432:5432 --network lms-network -e POSTGRES\_PASSWORD=password --name lmsdb postgres

1. Create Image lms-web FE
2. Create BE custom Image
3. DB

**SetUp FE:**

Build
```
docker build --tag lms-web .
```
```
docker run -d -p 80:80 --name lmsfe lms-web
```
**Setup Network:**

docker network create lms network

**Setup DB:**
```
docker run -d -p 5432:5432 --network lmsnetwork -e POSTGRES\_PASSWORD=password --name msdb postgres
```
**Setup BE:**

**Build BE:**
```
docker build --tag lms-public-api .
```
```
docker run -d -p 8080:8080 --network lmsnetwork -e DATABASE\_URL=postgresql://postgres:password@lmsdb:5432/postgres -e PORT=8080 -e MODE=local lms-public-api
```
```
Make sure we open port 8080
```
**Docker Volumes:**

Docker volumes are a way to save data generated by Docker containers in a place outside the container. This allows the data to persist even if the container is deleted or recreated. Volumes can be used to share data between containers and with the host machine, and they can be managed separately from the containers that use them.

There are three types of Docker volumes:

1. Host volumes: This type of volume maps a directory on the Docker host to a directory inside a container, allowing data to be shared between the two.
2. Anonymous volumes: These are temporary volumes that are created and managed by Docker. They are automatically deleted when the container is removed.
3. Named volumes: This type of volume is similar to a host volume, but it is managed by Docker and can be used across multiple containers. Named volumes provide a way to store persistent data that can be shared between containers and kept separate from the host file system.

docker run --name postgres -v /path/to/host/dir:/var/lib/postgresql/data -e POSTGRES\_PASSWORD=mysecretpassword -d postgres

This command creates a new Postgres container and mounts the /path/to/host/dir directory on the host file system to the /var/lib/postgresql/data directory inside the container.

docker run --name postgres -v /var/lib/postgresql/data -e POSTGRES\_PASSWORD=mysecretpassword -d postgres

This command creates a new Postgres container and creates an anonymous volume to store the data. The volume is automatically deleted when the container is removed.

docker volume create pgdata

docker run --name postgres -v pgdata:/var/lib/postgresql/data -e POSTGRES\_PASSWORD=mysecretpassword -d postgres

The first command creates a named volume called pgdata. The second command creates a new Postgres container and mounts the pgdata volume to the /var/lib/postgresql/data directory inside the container. This allows the volume to be reused **across multiple containers.**

**What is Docker Compose**

![image](https://user-images.githubusercontent.com/130965749/236377601-d0a7b4b4-9a12-4060-b507-b2870020e68a.png)

Docker Compose is a tool that allows developers/DevOPs to define and run multi-container Docker applications using a YAML file. It provides an easy way to manage and deploy complex applications that require multiple containers to run.

With Docker Compose, developers can define the different services that make up their application, along with their configurations and dependencies, in a single file called a "docker-compose.yml" file. Each service is defined as a separate container, and Docker Compose handles the orchestration of starting, stopping, and scaling these containers.

Some of the key features of Docker Compose include:

1. Multi-container orchestration: Docker Compose allows developers/DevOps to define and manage multiple containers that make up their application, enabling them to deploy complex, multi-tiered applications with ease.
2. Easy configuration: Developers/DevOPs can define the configurations for each service in the docker-compose.yml file, making it easy to manage and deploy their application with a single command.
3. Automatic network configuration: Docker Compose automatically creates a network for the containers to communicate with each other, enabling seamless communication between different services.
4. Scalability: Docker Compose makes it easy to scale the application up or down by adjusting the number of containers for each service.

Overall, Docker Compose simplifies the process of deploying and managing multi-container applications, making it a popular tool among developers/devops who are building complex applications using Docker.

**Docker Compose file:**

A Docker Compose file is a YAML file that defines a multi-container Docker application. It's used to define and run multiple Docker containers as a single service. A Compose file can be used to define a range of options and settings for each container in the application, such as networking, volumes, environment variables, and more.

Here's a breakdown of the different sections you might find in a typical Docker Compose file:

### **Version**

The version section specifies the version of the Docker Compose file format that the file is written in. This determines the syntax and structure of the file.

### **Services**

The services section is where you define the different containers that make up your application. Each service is defined as a separate block of code within the services section. You can specify the name of the service, the Docker image to use, any environment variables, ports to expose, and any other configuration options that the container needs.

### **Networks**

The networks section allows you to specify any networks that your containers need to connect to. You can define the name of the network, the driver to use, and any configuration options for the driver. For example:

**LMS Docker Compose File:**

The local-db service uses the official PostgreSQL Docker image as its base image. It sets some environment variables to configure the database, such as the username and password for the default user, and the name of the database to create. The ports section maps port 5432 on the container to port 5432 on the host machine, so the database can be accessed from outside the container.

The api-server service uses a custom Docker image called lms-public-api. It's built using the Dockerfile located in the api/ directory. The environment section sets environment variables that are used by the API server, such as the mode, port, and database URL. The ports section maps port 8080 on the container to port 8080 on the host machine. The restart option specifies that the container should be automatically restarted if it fails for any reason. Finally, the depends\_on section specifies that this service depends on the local-db service being up and running before it starts.

The web-server service uses a custom Docker image called lms-web. It's built using the Dockerfile located in the webapp/ directory. The environment section sets an environment variable called API\_URL, which tells the web server where to find the API server. The ports section maps port 80 on the container to port 80 on the host machine, so the web server can be accessed from a web browser. The depends\_on section specifies that this service depends on the api-server service being up and running before it starts.

There are also some commented out sections in this Docker Compose file that provide some additional configuration options. For example, there are two different ways to persist the data in the local-db container: using a bind mount or a named volume. These options are commented out, so they won't be used unless you uncomment them.

**Run LMS with Compose**

**Docker Compose Commands:**

1. docker-compose up: This command starts the containers defined in your docker-compose.yml file.
2. docker-compose down: This command stops and removes the containers defined in your docker-compose.yml file.
3. docker-compose build: This command builds the images for the services defined in your docker-compose.yml file.
4. docker-compose ps: This command lists the containers for the services defined in your docker-compose.yml file.
5. docker-compose logs: This command shows the logs for the services defined in your docker-compose.yml file.
6. docker-compose pull: This command pulls the latest image for the services defined in your docker-compose.yml file.
7. docker-compose push: This command pushes the built images for the services defined in your docker-compose.yml file to a registry.
8. docker-compose restart: This command restarts the containers defined in your docker-compose.yml file.
9. docker-compose stop: This command stops the containers defined in your docker-compose.yml file.
10. docker-compose exec: This command allows you to run a command in a running container. For example, docker-compose exec webserver ls -l /var/www.
11. docker-compose run: This command runs a one-time command in a new container. For example, docker-compose run --rm webserver bash.

**Namespaces:**

Namespacing in Linux is a feature that allows you to create isolated environments, almost like mini copies of the operating system. This means that the processes running inside a namespace have their own "view" of the system, and don't interfere with other processes or the host system.

For example, you could create a namespace that has its own set of network interfaces, process IDs, and file systems, so that the processes running within that namespace can't see or interact with the network interfaces, processes, or file systems outside of that namespace.

This is useful in many situations, but it's commonly used for containerization, which is a way of packaging applications and their dependencies into a single "container" that can be run in different environments without affecting the host system or other applications.

Namespacing is a feature in Linux that allows you to create isolated environments with their own view of the system resources. Docker is a popular containerization platform that uses namespacing to provide resource isolation between containers running on the same system.

Docker uses namespaces to create an isolated environment for each container, where each container has its own view of the system resources, such as its own network interfaces, process IDs, and file systems. This allows multiple Docker containers to run on the same system without interfering with each other or the host system.

For example, imagine you have two Docker containers running on the same system, each with their own web server application. By using namespaces, each container has its own view of the system resources, and the web server running in one container can't interact with the web server running in the other container.

Docker uses multiple types of namespaces, such as the PID namespace, which isolates the process IDs of the containers, and the network namespace, which isolates the network interfaces and routing tables of the containers. By using namespaces, Docker provides a lightweight and efficient way to run multiple applications on the same system while maintaining isolation and security between them.

**Control groups:**

Control groups, also known as cgroups, are a feature in Linux that allows you to control how much system resources (like CPU, memory, or I/O) are used by a group of processes.

For example, imagine you have two programs running on your computer, one of which is using a lot of CPU and slowing down the other program. By using cgroups, you could limit the CPU usage of the first program so that it doesn't slow down the other program.

Cgroups are also used to manage the resources used by containers, which are a way of running applications in an isolated environment. By using cgroups, you can ensure that one container doesn't use up too much of the system resources and affect the performance of other containers or the host system.

Overall, cgroups are a useful tool for managing system resources and ensuring that processes or containers don't use too many resources and impact other processes or the system as a whole.

Cgroups and Docker are closely related technologies. Cgroups are a feature in Linux that allows you to control the allocation of system resources (such as CPU, memory, and I/O) for a group of processes, while Docker is a popular containerization platform that allows you to package applications and their dependencies into a single container.

Docker uses cgroups to provide resource isolation between containers running on the same system. Each Docker container is assigned its own set of cgroups, which allows you to control the amount of CPU, memory, and other resources that the container can use. By using cgroups, you can ensure that one container doesn't use up too many system resources and impact the performance of other containers or the host system.

For example, if you have a system with limited resources, you could use cgroups to limit the CPU and memory usage of each Docker container, so that none of the containers use too much resources and slow down the others.

Overall, cgroups and Docker work together to provide resource isolation and management, making it easier to run multiple applications on the same system without affecting the performance of other applications or the system as a whole.

**Kubernetes :**

![image](https://user-images.githubusercontent.com/130965749/236377695-91923e8a-0813-4b7c-9079-65dcefb50602.png)

Kubernetes is an open-source container orchestration system for automating deployment, scaling, and management of containerized applications.

**Why Kubernetes :**

Kubernetes is a popular choice for container orchestration because it offers several key features and benefits:

1. Scalability: Kubernetes allows you to easily scale up or down the number of replicas of a containerized application, making it easy to handle changes in traffic or load.
2. High availability: Kubernetes can automatically detect when a containerized application is failing and automatically reschedule it on a healthy node, ensuring high availability.
3. Self-healing: Kubernetes monitors the state of the containers and the nodes they run on, and can automatically recover from failures, ensuring that your application stays up and running.
4. Portable: Kubernetes is platform-agnostic, making it easy to run and manage applications in different environments, such as on-premises, in the cloud, or in a hybrid environment.
5. Easy to deploy: Kubernetes provides a declarative configuration model, allowing you to easily deploy, update, or rollback your application with minimal effort.
6. Large community support: Kubernetes has a large and active community which provides a wide range of tools, plugins and support for the platform which makes it easy for the developers to work with it.

![image](https://user-images.githubusercontent.com/130965749/236377756-21177c3c-a07c-4f89-99bc-751f039878ca.png)

![image](https://user-images.githubusercontent.com/130965749/236377809-3ca4d662-8767-4143-9889-dfdad70e6637.png)

**Kubernetes Architecture :**
![image](https://user-images.githubusercontent.com/130965749/236377851-77ec6fb8-92ea-47ff-aa9f-58050622d4be.png)

![image](https://user-images.githubusercontent.com/130965749/236377897-26fe27b9-290c-46aa-b5fd-50197a7d57a3.png)

`

**Kubernetes architecture is composed of several key components, including:**

1. Master Node: The master node is the control plane of the Kubernetes cluster. It is responsible for maintaining the desired state of the cluster, scheduling and managing the placement of containers, and exposing the Kubernetes API.
2. Worker Node: Worker nodes are the machines that run the containerized applications. They communicate with the master node to receive instructions and report their status.
3. etcd: etcd is a distributed key-value store that stores the configuration data of the Kubernetes cluster, such as the desired state of the cluster, service discovery information, and secrets.
4. API Server: The API Server is the main entry point for all Kubernetes operations. It exposes the Kubernetes API and communicates with the etcd to read and write the configuration data.
5. Controller Manager: The Controller Manager is responsible for maintaining the desired state of the cluster. It watches the API server for changes and makes adjustments to the worker nodes to ensure that the desired state is achieved.
6. Scheduler: The Scheduler is responsible for scheduling pods on worker nodes. It takes into account the resource requirements of the pods and the availability of resources on the worker nodes.
7. Kubelet: Kubelet is an agent that runs on each worker node. It communicates with the API Server to receive instructions and reports the status of the node and the containers running on it.
8. Kubernetes Dashboard: The Kubernetes Dashboard is a web-based UI that allows you to manage and monitor the state of your Kubernetes cluster.
9. Kube-Proxy: Kube-Proxy is a network proxy and load balancer that runs on each worker node. It routes network traffic to the correct pod based on IP and port.

.

**Kubernetes Objects:**
![image](https://user-images.githubusercontent.com/130965749/236377948-2daa3994-02fc-4d25-a008-c506b824aabe.png)

Kubernetes objects are the basic building blocks of a Kubernetes cluster. They represent the desired state of the cluster and are used to define and manage the resources that make up an application. Some common types of Kubernetes objects include

1. Pods: A pod is the smallest and simplest unit in the Kubernetes object model. It represents a single instance of a running process in a cluster. Pods can contain one or more containers.
2. Replication Controllers/Replica Set : A Replication Controller ensures that a specified number of replicas of a Pod are running at any given time. If there are too few replicas, it will create new ones, and if there are too many, it will delete extras.
3. Services: A Service is an abstraction that defines a logical set of Pods and a policy by which to access them. Services enable network access to the Pods and enable load-balancing among them.
4. Deployments: A Deployment is a higher-level object that manages Replication Controllers and Pods. It allows you to declaratively manage the desired state of your application and roll out updates to your application with minimal downtime.
5. StatefulSets: A StatefulSet is a higher-level object that manages Pods with unique network identities and persistent storage. It is used to deploy stateful applications.
6. DaemonSets: A DaemonSet ensures that a copy of a Pod runs on all (or a set of) Nodes in the cluster.
7. ConfigMaps and Secrets: ConfigMaps and Secrets are used to store configuration data and sensitive information, respectively, that can be consumed by Pods.
8. Namespaces: Namespaces are used to partition a cluster into virtual clusters, each with its own resources and access controls.

**Kubernetes Pod :**

![image](https://user-images.githubusercontent.com/130965749/236377989-c1da1731-bfdd-4fbe-85bc-fb03aa4fc4a3.png)

![image](https://user-images.githubusercontent.com/130965749/236378049-f8b580c4-2ef4-4649-8b2d-d3dcb24a5b19.png)

![image](https://user-images.githubusercontent.com/130965749/236378092-e85739fa-3d45-49be-8246-56dd184d98ba.png)

#### **YAML for Kubernetes:**

[Kubernetes](https://www.redhat.com/en/topics/containers/what-is-kubernetes) works based on defined state and actual state. Kubernetes objects represent the state of a cluster and tell Kubernetes what you want the workload to look like. Kubernetes resources, such as pods, objects, and deployments can be created using YAML files.

When creating a Kubernetes object, you'll need to include specifications to define the object's desired state. The [Kubernetes API](https://www.redhat.com/en/topics/containers/what-is-the-kubernetes-API) can be used to create the object. The request to the API will include the object specifications in JSON, but most often you'll provide the required information to kubectl as a YAML file. Kubectl will convert the file into YAML for you when it makes the API request.

Once an object has been created and defined, Kubernetes works to make sure that the object always exists.

Developers or sysadmins specify the defined state using the YAML or JSON files they submit to the Kubernetes API. Kubernetes uses a controller to analyze the difference between the new defined state and the actual state in the cluster.

Each object defined in a YAML file is made up of several different fields.

In this example, the

apiVersion,

Kind

Metadata

Spec

The spec field in this example contains several other fields, such as replicas, selector, and template, which are used to define the details of the deployment.

You can use the kubectl command-line tool to create, update, and delete objects defined in YAML files. For example, to create the deployment defined in this example, you would run the command kubectl create -f deployment.yaml.

You can also add various attributes like labels, annotations etc which can be used to identify the objects or any other context.

**Kubernetes API Version**

The apiVersion field in a Kubernetes resource configuration file specifies the version of the Kubernetes API that the resource belongs to. It is important to choose the correct apiVersion for your resource because different API versions can have different fields and behaviors.

Here are the steps to get the Kubernetes API version for a resource:

1. Make sure you have kubectl installed and configured to access your cluster. You can download it from the[official Kubernetes page](https://kubernetes.io/docs/tasks/tools/install-kubectl/).
2. Use the kubectl api-resources command to list all available resources along with their API groups and versions:
```
kubectl api-resources --output=wide
```
```
kubectl api-resources --output=wide | grep deployments
```
**Metadata in Kubernetes YAML File:**

In a Kubernetes YAML file, the metadata field is used to provide additional information about the object being defined. The metadata field is a required field for all Kubernetes objects and it contains several subfields that can be used to provide different types of information.

Some of the common fields in the metadata field are:

- name: The name of the object. This field is required and must be unique within the namespace of the object.
- namespace: The namespace in which the object will be created. If not specified, the object will be created in the default namespace.
- labels: A key-value map of labels that can be used to organize and identify the object.
- annotations: A key-value map of annotations that can be used to provide additional information about the object.
- generateName: A prefix to be used to generate the name of the object. If a name field is not specified, Kubernetes will automatically generate a name for the object using the value of generateName as a prefix.

**Specifications:**

In a Kubernetes YAML file, "specs" refers to the specifications or desired state of the Kubernetes resource being defined in the YAML file.

Every Kubernetes resource has a set of properties that define its desired state, such as the container image to use, the number of replicas to run, the resource limits, and so on. These properties are specified in the YAML file under the "spec" section, which is short for "specification".

**What are Pods in Kubernetes ?**

In Kubernetes, a pod is the basic unit of deployment. It is the smallest and simplest unit in the Kubernetes object model, and it is the unit of replication in a Kubernetes cluster.

A pod consists of one or more containers that are deployed together on the same host. The containers in a pod share the same network namespace, storage resources, and lifecycle. This means that they can communicate with each other using localhost and share data through shared volumes.

Pods are designed to be ephemeral, which means that they are expected to be replaced frequently. When a pod is deleted or restarted, it is replaced with a new pod, which means that the IP address and other network identifiers of the pod can change.

Pods are used to host applications and their dependencies, and they are the basic building blocks of a Kubernetes cluster. If you are familiar with Docker, you can think of a pod as a group of Docker containers that are deployed together on the same host.

**Nginx Pod Demo :**

apiVersion: v1
```
kind: Pod
```
metadata:
```
name: nginx-pod
```
spec:

containers:
```
- name: nginx
```
```
image: nginx:latest
```
ports:

- containerPort: 80

kubectl apply -f nginx-pod.yaml

kubectl get pods

kubectl describe pod nginx-pod

#### _ **POD** _

**Create an NGINX Pod**
```
kubectl run nginx --image=nginx
```
**Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)**
```
kubectl run nginx --image=nginx --dry-run=client -o yaml
```
```
kubectl apply -f nginx-pod.yaml
```
```
​​$kubectl get pods
```
```
kubectl run nginx --image=nginx:latest --port=80
```
**To get ip address**
```
$minikube ip
```
**To Display all pods and services**
```
$kubectl get all
```
**To test**
```
$ curl http://ipaddes:port
```
**Port forward command**
```
$ kubectl port-forward --address 0.0.0.0 service/\<servicename\> \<nodeportnumber\>:80
```
**Detail of pod**
```
$ kubectl describe \<podname\>
```
**Detail of pod**
```
$ kubectl describe \<podname\>
```
#### _ **POD** _

**Create an NGINX Pod**
```
kubectl run nginx --image=nginx
```
**Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)**
```
kubectl run nginx --image=nginx --dry-run=client -o yaml
```
#### _ **Deployment** _

**Create a deployment**
```
kubectl create deployment --image=nginx nginx
```
**Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)**

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

This YAML file defines a pod with a single container running the latest version of the Nginx image.

The container listens on port 80 and is given the name "nginx-container".

To create the pod, you can use the kubectl command-line tool:

**What are Replica Set ?**

In Kubernetes, a ReplicaSet is an object that is used to manage a set of identical pods. It ensures that a specified number of replicas of a pod are running at all times. If a pod fails or is terminated for any reason, the ReplicaSet automatically replaces it with a new one to maintain the desired number of replicas.

A ReplicaSet uses a selector to identify the pods that it manages. The selector is defined in the ReplicaSet's configuration and is used to match the labels that are applied to the pods. The ReplicaSet also specifies the number of replicas that should be created and maintained.

ReplicaSets are often used in conjunction with Deployments, which provide a higher-level abstraction for managing the deployment and scaling of applications. Deployments manage ReplicaSets, and the ReplicaSets in turn manage the pods.

ReplicaSets provide a way to ensure high availability and fault tolerance for applications running in Kubernetes. They can be used to scale an application horizontally by adding or removing replicas based on the workload, and they can also be used to perform rolling updates or rollbacks of an application's containers.

**Example :**

To create the ReplicaSet, you can use the kubectl command-line tool:

**Labels:**

In Kubernetes, labels are key-value pairs that can be attached to objects, such as pods, services, and deployments, to provide additional information about the object. Labels can be used for a variety of purposes, such as identifying the owner of an object, providing information about the environment in which an object is running, or indicating the version of an object.

In a YAML file, labels are defined in the metadata field of an object. Here's an example of how labels are defined in a deployment YAML file:

Selectors are used to filter Labels

Labels are a powerful feature of Kubernetes that enable users to organize and manage their resources in a flexible and scalable way. By using labels to group related resources, users can create complex applications and services that can be managed and scaled more easily.

Kubectl get pods –selector app=App1

**Selectors in kubernetes**

In Kubernetes, selectors are used to filter and select a group of objects based on their labels. Selectors are used in many Kubernetes objects, such as ReplicaSets, Services, and Deployments, to determine which objects should be affected by the object's configuration.

A selector is a label query, which consists of a set of key-value pairs that are matched against the labels of the objects in the cluster. In a YAML file, selectors are defined in the spec field of an object, usually in the selector field, as well as other fields like matchLabels, matchExpressions depending on the object.

**What is Deployment ?**

A deployment in general refers to the process of releasing and updating software applications, services, and infrastructure in a controlled and predictable manner. It involves creating, testing, and delivering software updates to various environments such as development, QA, and production. The goal of a deployment is to ensure that new changes are made available to end-users in a reliable and consistent manner, while minimizing the impact of any issues that may arise during the process.

The method of deployment can vary depending on the type of application, the infrastructure it runs on, and the organization's development and operations processes. There are different strategies of deployment such as Blue-Green Deployment, Rolling Deployment, Canary Deployment, A/B Deployment etc. which can be used depending on the use case and requirements.

**Deployments in Kubernetes:**

To deploy an application in Kubernetes, you will need to create a deployment configuration that specifies the application container image, the number of replicas of the application to run, and other details. The deployment configuration can be written in YAML or JSON and is used to create a deployment object in the Kubernetes API.

Once the deployment object is created, Kubernetes will ensure that the specified number of replicas of the application are running and available. If an application replica goes down, Kubernetes will automatically restart it. You can also use Kubernetes to scale the number of replicas up or down as needed.

In Kubernetes, a deployment is a way to describe a desired state for a group of replicated applications, and it helps ensure that the desired state is actualized. A deployment is a higher-level concept than a pod, which is a single instance of an application in Kubernetes.

A deployment is responsible for creating and updating a set of replicas of your application, which are defined as pods. When you create a deployment, you specify the container image for your application and the number of replicas you want to run. The deployment will then ensure that the specified number of replicas are running and available at all times. If a replica goes down, the deployment will automatically restart it.

Deployments are useful because they provide an easy way to declaratively manage the lifecycle of your applications. You can use deployments to perform rolling updates, rollback updates, or pause and resume updates to your applications.

kubectl apply -f deployment.yaml

Update and RollBack in Deployment:

Revision 1: Nginx 10

Revision 2:

Kubectl rollout status

Deployment Statrgy

Recreate ( not Default)

Rolling Updates (Defalut)

Kubectl apply -f after changes

Kubectl describe deployment

apiVersion: apps/v1

kind: Deployment

metadata:

name: nginx-deployment

spec:

replicas: 3

selector:

matchLabels:

app: nginx

template:

metadata:

labels:

app: nginx

spec:

containers:

- name: nginx-container

image: nginx

ports:

- containerPort: 80

#### _ **Deployment** _

**Create a deployment**

kubectl create deployment --image=nginx nginx

**Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)**

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

**Generate Deployment with 4 Replicas**

kubectl create deployment nginx --image=nginx --replicas=4

**Another way to do this is to save the YAML definition to a file and modify**

kubectl create deployment nginx --image=nginx --dry-run=client -o yaml \> nginx-deployment.yaml

You can then update the YAML file with the replicas or any other field before creating the deployment.

**There are several types of deployments in Kubernetes:**

1. Rolling deployment: This type of deployment updates the application by replacing the old version of the application with the new version, one pod at a time. This allows you to update your application with minimal downtime.

**Blue-green deployment:**

This type of deployment involves creating a parallel copy of your application, called the "green" version, and switching traffic to the green version when it is ready. This allows you to test the new version of the application before making it live, and it also allows you to easily roll back to the old version if needed.

Blue-Green deployment is a technique that allows you to deploy a new version of an application alongside the existing version, and then switch traffic between the two versions. The basic idea is to run two identical environments, one in production which is the "green" environment and another one which is the "blue" environment.

Here is an example of how a Blue-Green deployment pipeline might look like in a high level:

1. The "blue" environment is the current production environment where the application is running and serving live traffic.
2. The "green" environment is a new environment that is created alongside the "blue" environment.
3. The new version of the application is deployed to the "green" environment, it goes through a series of tests to ensure that it's working as expected.
4. Once the new version is deemed stable, traffic is redirected from the "blue" environment to the "green" environment.
5. After the traffic is switched, the old "blue" environment can be decommissioned or kept as a fallback option.
6. The process can be repeated with new versions of the application being deployed to the "green" environment while traffic is still being served by the "blue" environment.

Please note that the above-described pipeline is just an example and there are various ways to implement blue-green deployments in different environments and with different tools. However, the main idea is to have two identical environments running in parallel and the ability to switch traffic between them.

1. Canary deployment: This type of deployment involves releasing a new version of the application to a small group of users, and gradually rolling it out to more users if it is successful. This allows you to test the new version of the application before making it available to all users.

Canary deployment is a technique used to gradually roll out new changes to a small subset of users before releasing the changes to the entire user base. This approach allows you to test new changes in a production-like environment and detect and fix any issues before they affect the entire user base.

Here is an example of how you might implement Canary deployment in Kubernetes:

1. Create two identical Replication Controllers, one for the existing version of the application (the "stable" version) and one for the new version of the application (the "canary" version).
2. Create two Services, one for the "stable" version and one for the "canary" version. The Services should have different selectors to route traffic to the appropriate Replication Controllers.
3. Initially, route all traffic to the "stable" version of the application using a Kubernetes Ingress or a LoadBalancer.
4. Deploy the new "canary" version of the application by scaling up the "canary" Replication Controller and ensuring that all of its Pods are running and healthy.
5. Gradually route a small percentage of traffic to the "canary" version of the application by configuring the Ingress or LoadBalancer to split traffic between the "stable" and "canary" Services.
6. Monitor the "canary" version of the application for any issues and roll back to the "stable" version if necessary.
7. Once the "canary" version of the application is stable, gradually increase the percentage of traffic routed to the "canary" version until all traffic is being served by the new version.
8. Once the new version is fully adopted, the "stable" version can be decommissioned

It's worth noting that Canary deployment can be implemented in many different ways and this example is just one possible approach. Additionally, some other factors such as service discovery and load balancing should be considered when implementing the Canary deployment in a production environment.

In general, a Canary Deployment image would show the different components of a Canary Deployment pipeline, such as the two identical environments (canary and stable), the traffic routing mechanism (ingress, load balancer, etc.), and the monitoring and rollback mechanisms.

The image would have two identical environments, the "canary" environment and the "stable" environment. The "canary" environment would be running the new version of the application, while the "stable" environment would be running the existing version.

A traffic routing mechanism would be shown, such as an ingress controller or a load balancer, which would be responsible for directing a small percentage of traffic to the "canary" environment.

The image would also include monitoring and rollback mechanisms, such as logging and monitoring systems that would be used to observe the performance of the "canary" environment and detect any issues. If issues are detected, the image would show how traffic can be quickly rolled back to the "stable" environment.

Once the new version of the application has been deemed stable, the image would show how the percentage of traffic to the "canary" environment can be gradually increased until all traffic is being served by the new version, and the old version can be decommissioned.

1. Recreate deployment: This type of deployment involves replacing all replicas of the application at once. This can be useful if you need to perform a major update to the application that cannot be done using a rolling update. However, this can cause downtime for the application, so it should be used with caution.

1. kubectl create deployment: This command creates a new Deployment in the cluster. You can specify the desired number of replicas, the container image to use, and other options.
2. kubectl get deployments: This command retrieves information about one or more Deployments in the cluster. You can use the -o option to specify the output format, such as JSON or YAML.
3. kubectl describe deployment: This command provides detailed information about a specific Deployment, including the number of replicas, the container image being used, and the current status of the Deployment.
4. kubectl scale deployment: This command allows you to scale the number of replicas in a Deployment.
5. kubectl edit deployment: This command allows you to edit the configuration of an existing Deployment.
6. kubectl rollout status deployment: This command checks the status of a Deployment's rollout.
7. kubectl rollout undo deployment: This command allows you to undo the last rollout of a Deployment.
8. kubectl delete deployment: This command deletes a Deployment from the cluster, including all associated replicas.

**Networking in Kubernetes:**

In Kubernetes, networking is the communication between different pods and services within a cluster. Each pod is assigned a unique IP address and can communicate with other pods and services using this IP. Services, on the other hand, provide a stable endpoint for pods and can load balance traffic between multiple pods. Kubernetes uses a system called kube-proxy to handle network communication between pods and services. Additionally, Kubernetes also provides an ingress resource that allows external traffic to access services within a cluster.

In Kubernetes, a service is a way to access one or more pods in a cluster. Services provide a stable endpoint for pods, which can change IP addresses as they are created, deleted or moved within the cluster. Services can also load balance traffic between multiple pods.

**There are three types of Services in Kubernetes:**

1. ClusterIP: Exposes the service on a cluster-internal IP. This type of service is only accessible from within the cluster.
2. NodePort: Exposes the service on each node's IP at a static port. This type of service is accessible from outside the cluster using the node IP and port.
3. LoadBalancer: Exposes the service externally using a cloud provider's load balancer. This type of service is accessible from outside the cluster using the load balancer's IP and port.

Additionally, Kubernetes also provides an ingress resource that allows external traffic to access services within a cluster.

![image](https://user-images.githubusercontent.com/130965749/236378187-108a627c-d8d0-4e27-8870-fdd295bf047e.png)

![image](https://user-images.githubusercontent.com/130965749/236378239-3b0cd4ca-65b9-4a73-837a-07dae0fe2ca0.png)

**NodePort :**

A NodePort is the simplest networking type of all. It requires no configuration, and it simply routes traffic on a random port on the host to a random port on the container. This is suitable for most cases, but it does have some disadvantages:

- You may need to use a reverse proxy (like Nginx) to ensure that web requests are routed correctly.
- You can only expose one single service per port.
- Container IPs will be different each time the pod starts, making DNS resolution impossible.
- The container cannot access localhost from outside of the pod, as there is no IP configured.

Nevertheless, you can use NodePort during experimentation and for temporary use cases, such as demos, POCs, and internal training to show how traffic routing works. It is recommended **not** to use NodePort in production to expose services.

##
## **LoadBalancer**

LoadBalancer is the most commonly used service type for Kubernetes networking. It is a standard load balancer service that runs on each pod and establishes a connection to the outside world, either to networks like the Internet, or within your datacenter.

The LoadBalancer will keep connections open to pods that are up, and close connections to those that are down. This is similar to what you have on AWS with ELBs, or Azure with Application Gateway. Upstreams provide Layer 4 routing for HTTP(S) traffic, whereas Downstreams provide Layer 7 routing for HTTP(S) traffic.

You can route traffic on destination port number, protocol and hostname, or use application labels. You can send almost any kind of traffic to this service type, such as HTTP, TCP, UDP, Grpc, and more. Use this approach to expose your services directly.

##
# **Ingress**

Ingress is not considered an official Kubernetes service, but it can be used to expose services. You can configure an Ingress service by creating rules to define which inbound connections should reach which services.

An Ingress is a Kubernetes object that sits in front of multiple services and acts as an intelligent router. It defines how external traffic can reach the cluster services, and it configures a set of rules to allow inbound connections to reach the services on the cluster.

## Conclusion: Kubernetes is a Must in the Cloud-Native World

#### _ **Service** _

**Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379**

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)

Or

kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml (This will not use the pods labels as selectors, instead it will assume selectors as **app=redis.** [You cannot pass in selectors as an option.](https://github.com/kubernetes/kubernetes/issues/46191) So it does not work very well if your pod has a different label set. So generate the file and modify the selectors before creating the service)

**Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:**

kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors,[but you cannot specify the node port](https://github.com/kubernetes/kubernetes/issues/25478). You have to generate a definition file and then add the node port in manually before creating the service with the pod.)

Or

kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml

(This will not use the pods labels as selectors)

Both the above commands have their own challenges. While one of it cannot accept a selector the other cannot accept a node port. I would recommend going with the kubectl expose command. If you need to specify a node port, generate a definition file using the same command and manually input the nodeport before creating the service.

Deployment.yaml

apiVersion: apps/v1

kind: Deployment

metadata:

name: web

labels:

app: frontend

spec:

replicas: 3

selector:

matchLabels:

app: frontend

template:

metadata:

labels:

app: frontend

spec:

containers:

- name: frontend

image: nginx

ports:

- containerPort: 80

kubectl apply -f deployment.yaml

To check the from container using imperative command

$ kubectl exec \<podname\> -- cat /usr/share/nginx/html/index.html

Now Create a Service

|
 |
 |
| --- | --- |
|
 |
 |

apiVersion: v1

kind: Service

metadata:

name: frontend-service

spec:

selector:

app: frontend

ports:

- name: http

protocol: TCP

port: 80

targetPort: 80

type: NodePort

Apply the service

$ kubectl apply -f web-service.yaml

Exposing externally

$ kubectl port-forward –address 0.0.0.0 service/servicename nodeportnumber:containerportnumber

Now We can access from out side the cluster

##


##


##
# Kubernetes Services

There are four types of services that Kubernetes supports:

· **ClusterIP**

· **NodePort**

· **LoadBalancer and**

· **Ingress**.

Each has their own set of requirements to enable them for your application, so you must understand which one you need before deploying.

## **ClusterIP**

ClusterIP is the default service that enables the communication of multiple pods within the cluster. By default, your service will be exposed on a ClusterIP if you don't manually define it. ClusterIP can't be accessed from the outside world. But, a Kubernetes proxy can be used to access your services. This service type is used for internal networking between your workloads, while debugging your services, displaying internal dashboards, etc.

**NodePort :**

A NodePort is the simplest networking type of all. It requires no configuration, and it simply routes traffic on a random port on the host to a random port on the container. This is suitable for most cases, but it does have some disadvantages:

- You may need to use a reverse proxy (like Nginx) to ensure that web requests are routed correctly.
- You can only expose one single service per port.
- Container IPs will be different each time the pod starts, making DNS resolution impossible.
- The container cannot access localhost from outside of the pod, as there is no IP configured.

Nevertheless, you can use NodePort during experimentation and for temporary use cases, such as demos, POCs, and internal training to show how traffic routing works. It is recommended **not** to use NodePort in production to expose services.

##
## **LoadBalancer**

LoadBalancer is the most commonly used service type for Kubernetes networking. It is a standard load balancer service that runs on each pod and establishes a connection to the outside world, either to networks like the Internet, or within your datacenter.

The LoadBalancer will keep connections open to pods that are up, and close connections to those that are down. This is similar to what you have on AWS with ELBs, or Azure with Application Gateway. Upstreams provide Layer 4 routing for HTTP(S) traffic, whereas Downstreams provide Layer 7 routing for HTTP(S) traffic.

You can route traffic on destination port number, protocol and hostname, or use application labels. You can send almost any kind of traffic to this service type, such as HTTP, TCP, UDP, Grpc, and more. Use this approach to expose your services directly.

##
# **Ingress**

Ingress is not considered an official Kubernetes service, but it can be used to expose services. You can configure an Ingress service by creating rules to define which inbound connections should reach which services.

An Ingress is a Kubernetes object that sits in front of multiple services and acts as an intelligent router. It defines how external traffic can reach the cluster services, and it configures a set of rules to allow inbound connections to reach the services on the cluster.

## Conclusion: Kubernetes is a Must in the Cloud-Native World

## **Create Kubernetes Cluster With minikube Lab**

**Steps 1:** _ **create a Ubuntu Server name for example k8s server** _

**Steps 2:** _ **install Docker in k8s server** _

**Steps 3:** _ **install minikube on ubtuntu** _

**Steps 4:** _ **clone the repository lms-public** _[https://github.com/digitaledify/lms-public.git](https://github.com/digitaledify/lms-public.git)

**Steps 5:** _ **Build the frontend image and login , push to docker hub** _

**Steps 7:** _ **write a frontend deployment file and service file.** _

**Steps 8:** _ **install kubectl.** _

**Steps 8** _ **. create a cluster.** _

**Steps 8** _ **. Finally deploy the pod on the cluster which you have created.** _

_ **Solution;** _

_ **Lunch a instance with ubuntu 20 version** _

_ **Connect to the instance install** _

·_ **Update and upgrade** _

·_ **Minikube** _

·_ **Kubectl** _

·_ **Docker** _

·_ **Wget** _

_ **Minikube installation** _

[https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/)

##


## Installation

curl-LO https://storage.googleapis.com/minikube/releases/latest/minikube\_latest\_amd64.deb

sudo dpkg -i minikube\_latest\_amd64.deb

Start your cluster

From a terminal with administrator access (but not logged in as root), run:

$minikube start

check the status with status command

**Install kubectl on linux**

curl -LO[https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl](https://dl.k8s.io/release/%24(curl%20-L%20-s%20https:/dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl)

chmod +x kubectl

sudo mv kubectl /usr/local/bin/kubectl

Goto frontend and build the frontend image

$docker build -t mubeen507/lms-web .

##
# **Namespaces**

In Kubernetes, _namespaces_ provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects _(e.g. Deployments, Services, etc)_ and not for cluster-wide objects _(e.g. StorageClass, Nodes, PersistentVolumes, etc)_.

## When to Use Multiple Namespaces

Namespaces are intended for use in environments with many users spread across multiple teams, or projects. For clusters with a few to tens of users, you should not need to create or think about namespaces at all. Start using namespaces when you need the features they provide.

Namespaces provide a scope for names. Names of resources need to be unique within a namespace, but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.

Namespaces are a way to divide cluster resources between multiple users (via [resource quota](https://kubernetes.io/docs/concepts/policy/resource-quotas/)).

It is not necessary to use multiple namespaces to separate slightly different resources, such as different versions of the same software: use [labels](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels) to distinguish resources within the same namespace.

##
# **DaemonSet**

A _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:

- running a cluster storage daemon on every node
- running a logs collection daemon on every node
- running a node monitoring daemon on every node

In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon. A more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and cpu requests for different hardware types.

##
# **ConfigMaps**

A ConfigMap is an API object used to store non-confidential data in key-value pairs. [Pods](https://kubernetes.io/docs/concepts/workloads/pods/) can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a [volume](https://kubernetes.io/docs/concepts/storage/volumes/).

A ConfigMap allows you to decouple environment-specific configuration from your [container images](https://kubernetes.io/docs/reference/glossary/?all=true#term-image), so that your applications are easily portable.

##
# **Secrets**

A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a [Pod](https://kubernetes.io/docs/concepts/workloads/pods/) specification or in a [container image](https://kubernetes.io/docs/reference/glossary/?all=true#term-image). Using a Secret means that you don't need to include confidential data in your application code.

Because Secrets can be created independently of the Pods that use them, there is less risk of the Secret (and its data) being exposed during the workflow of creating, viewing, and editing Pods. Kubernetes, and applications that run in your cluster, can also take additional precautions with Secrets, such as avoiding writing secret data to nonvolatile storage.

Secrets are similar to [ConfigMaps](https://kubernetes.io/docs/concepts/configuration/configmap/) but are specifically intended to hold confidential dat

